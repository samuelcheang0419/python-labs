{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import requests\n",
    "import itertools\n",
    "import random\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Standard Library and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Wow**. Just. Wow. This is our last lab, so I can't help but be a little sentimental. Just think about how far we've come together, folks. You've learned so much about the Python programming language by this point. You've learned about its philosophy and decisions. Hopefully, you've learned why those decisions make it such a popular, amazing language. You've hopefully also learned about Python's drawbacks. \n",
    "\n",
    "You've learned how Python's core philosophy integrates into every feature of the language. We've talked about Python's data structures, functional programming in Python, object-oriented Python, error handling, decorators, iterators, generators, third-party and standard library tools. We've studied machine learning and learned about Python in data science.\n",
    "\n",
    "And, you put all of that together to triangulate the location of a freaking unicorn.\n",
    "\n",
    "Just think about that.\n",
    "\n",
    "Like, that's really advanced stuff.\n",
    "\n",
    "If you took the `triangulate.py` route, you eliminated noise from data to triangulate latitude/longitude coordinates of an actual stuffed unicorn, hidden on campus. And it's not like we controlled the noise at all: the data we generated was **really** noisy. Every data point was generated with a variance of several kilometers.\n",
    "\n",
    "If you walked through the Row of Puzzles, you've used a truly dazzling array of Python language features. You wrote a decorator to inspect a strange function, you used `requests` and `numpy` to piece together *audio* files! That's freaking amazing.\n",
    "\n",
    "Take a pause, breathe, and pat yourself on the back.\n",
    "\n",
    "In Part 1 of this lab, we'll start with a variation on a problem that we promised you on the first day of class: the Netflix Recommendation Algorithm. Then, we hope you'll take some time to become more familiar with the tools provided by Python's standard library. We want you to gain practice with the most common utilities of the standard library and also to be aware of the rest of the tools in case you ever need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code\n",
    "Download the starter code for this lab [here](https://stanfordpython.com/res/starter-code/lab-8.zip). Unzip those files and place them into the same directory as this lab file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netflix Recommendations\n",
    "A big part of Netflix's business strategy is being able to predict what kind of movies a user will enjoy watching, based on how they rate previous movies they've seen. This is such a big deal to the company that they offered [one million dollars as a prize](https://en.wikipedia.org/wiki/Netflix_Prize) to anyone who could beat their algorithm.\n",
    "\n",
    "Let's put this problem in formal terms. Suppose you have a matrix, where each row represents a user and each column represents a movie, like this one:\n",
    "![A matrix with rows representing users, columns representing movies, and the entries representing a user's movie for a specific rating. This matrix contains ratings by Parth, Michael, Joy, and Unicornelius for the movies Harry Potter and the Goblet of Fire, Unicorn Killer, Inside Out, and Frozen 2](https://raw.githubusercontent.com/stanfordpython/python-labs/master/notebooks/lab-8/movie_matrix.png)\n",
    "\n",
    "One version of the Netflix problem (which is the one that the prize money was for) is to *complete* the matrix. You'll notice that some of the boxes above are marked with `?`. That indicates that we don't have a rating from that user for that movie. Netflix offered a million dollars to any algorithm that could predict the values of those question markes better than their algorithm.\n",
    "\n",
    "That's a super interesting problem, and if you have some linear algebra background, I'd highly recommend that you read about it. I really like [Carlos Fernandez-Granda's notes on low-rank matrix completion](https://cims.nyu.edu/~cfgranda/pages/OBDA_spring16/material/low_rank_models.pdf).\n",
    "\n",
    "Today, we're going to implement a different, but related algorithm. We're going to write an algorithm that, **given a new user's preferences about movies, can suggest which movies that user is likely to enjoy**.\n",
    "\n",
    "Let's start with an observation: In the above matrix, look at the ratings for Inside Out and Frozen 2:\n",
    "```\n",
    "              Inside Out   Frozen 2\n",
    "Parth             5           4\n",
    "Michael           ?           1\n",
    "Joy               2           2\n",
    "Unicornelius      5           5\n",
    "```\n",
    "\n",
    "Notice that every user that rated both movies rated them pretty similarly (i.e., the values in the two columns are very close to each other). Based on that, we can conclude that Inside Out is pretty similar to Frozen 2, and if you like one movie, you'll probably like the other. Similarly, if you hate one movie, you'll probably hate the other.\n",
    "\n",
    "We'll formalize and compute the \"closeness\" of movies using **cosine similarity**. But first, let's load our data. <br />\n",
    "*This data comes from CS 124: From Languages to Information*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `load_data()`\n",
    "We've stored the data for this problem in two files: `movies.txt`, which has information about each of the movies that we've recorded and `ratings.txt`, which has information about how users rated each of the movies.\n",
    "\n",
    "In `load_data`, you should open the `ratings.txt` file, and extract the data into a matrix of the form that we depicted above, with users on the `0` axis and movies on the `1` axis. If a user hasn't rated a movie, leave that value as 0 in the matrix.\n",
    "\n",
    "#### `movies.txt`\n",
    "The `movies.txt` file looks like the following:\n",
    "```\n",
    "0%Toy Story (1995)%Adventure|Animation|Children|Comedy|Fantasy\n",
    "1%Jumanji (1995)%Adventure|Children|Fantasy\n",
    "2%Grumpier Old Men (1995)%Comedy|Romance\n",
    "3%Waiting to Exhale (1995)%Comedy|Drama|Romance\n",
    "4%Father of the Bride Part II (1995)%Comedy\n",
    "...\n",
    "```\n",
    "\n",
    "Each line is formatted like\n",
    "```\n",
    "id%name%categories\n",
    "```\n",
    "\n",
    "We're only interested in the `id` and the `name`. You can ignore the third field. **There are a total of 9125 movies, numbered from 0 to 9124.** Note that you may not need to open this file to load the data.\n",
    "\n",
    "#### `ratings.txt`\n",
    "The `ratings.txt` file looks like this:\n",
    "```\n",
    "0%30%2.500000\n",
    "...\n",
    "0%1962%2.500000\n",
    "0%2380%1.000000\n",
    "0%2925%3.000000\n",
    "1%9%4.000000\n",
    "1%16%5.000000\n",
    "1%37%5.000000\n",
    "...\n",
    "```\n",
    "\n",
    "Each line has the format:\n",
    "```\n",
    "user_id%movie_id%rating\n",
    "```\n",
    "\n",
    "That is, user number `user_id` rated movie number `movie_id` as `rating` between 1 and 5. **There are a total of 671 users, numbered 0 to 670**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "528.1296572280179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "    ratings_file = 'ratings.txt'\n",
    "    with open(ratings_file, 'r') as f:\n",
    "        ratings = np.array([line.split('%') for line in f])\n",
    "    movies_file = 'movies.txt'\n",
    "    with open(movies_file, 'r') as f:\n",
    "        movies = np.array([line.split('%')[:2] for line in f])\n",
    "    num_users = len(set(ratings[:, 0]))\n",
    "    num_movies = len(set(movies[:, 0]))\n",
    "    matrix = np.zeros((num_users, num_movies))\n",
    "    matrix[ratings[:, 0].astype(int), ratings[:, 1].astype(int)] = ratings[:, 2]\n",
    "    return matrix\n",
    "    \n",
    "ratings = load_data()\n",
    "\n",
    "# As a sanity check, we've pre-computed the expected value of this next line:\n",
    "np.mean(np.sum(ratings, axis=1)) # => 528.1296572280179"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `clean_data(ratings)`\n",
    "Great! We've got our data loaded! Now, let's clean it. For cosine similarity, we need each column to have norm 1. That is, it's length, as a 9125-dimensional vector, should be 1. Recall that the length of a vector is the square root of the sum of its entries, squared (this is the Pythagorean Theorem, also called the Euclidean norm). For example, if $x = (x_1, x_2, \\dots, x_n)$, then\n",
    "$$\\lVert x \\rVert = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}$$\n",
    "\n",
    "You can compute the norm of a vector using `np.linalg.norm`. That function also supports an `axis` keyword argument, which allows you to compute the norm \"along a given axis,\" to use Michael's terminology. **Be careful:** some movies don't have ratings, so their norm will be 0. To avoid a divide-by-zero issue, leave those columns untouched. It might help to treat their norms as though they're 1, so you don't modify their values when renormalizing.\n",
    "\n",
    "For example, if we normalize the Inside Out and Frozen 2 data from above, we'll get:\n",
    "```\n",
    "              Inside Out   Frozen 2\n",
    "Parth           0.680       0.589  \n",
    "Michael         0           0.147  \n",
    "Joy             0.272       0.294  \n",
    "Unicornelius    0.680       0.737  \n",
    "```\n",
    "\n",
    "Write a function, `clean_data(ratings)` that will take in the ratings matrix, and return a new matrix, where each column has norm 1.\n",
    "\n",
    "*Challenge: Try to implement this function without using any loops, using `numpy` broadcasting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.85379822810301"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(ratings):\n",
    "    rows, cols = ratings.shape\n",
    "    norm = np.linalg.norm(ratings, axis = 0)\n",
    "    divide_mask = np.ones((rows, cols), dtype = bool)\n",
    "    divide_mask[:, norm == 0] = False\n",
    "    return np.divide(ratings, norm.reshape((1, cols)), where = divide_mask)\n",
    "    \n",
    "normalized_ratings = clean_data(ratings)\n",
    "\n",
    "# As a sanity check, we've pre-computed the expected value of this next line:\n",
    "np.mean(np.sum(normalized_ratings, axis=1)) # => 32.85379822810301"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `suggest_movies(user_ratings, normalized_ratings, n=5)`\n",
    "Given a user's rating of all of the movies (i.e., a 9125-dimensional vector with entries between 0 and 5, where 0s represent un-rated movies), this function will return the indices of the **top `n` movies** that match with that user. We'll do this using cosine similarity.\n",
    "\n",
    "First, we'll compute a `movie_profile` for each user, which will be a 671-dimensional vector that combines the ratings we received as input with the ratings from the rest of the users. We do this by scaling each column of the matrix by the user's rating of that movie and then adding together all of the columns. For example, if the user rated Inside Out as a 4, Frozen 2 as a 3, and didn't rate any other movies, their profile would be:\n",
    "\n",
    "```\n",
    "              Inside Out      Frozen 2\n",
    "Parth          0.680 * 4  +  0.589 * 3 = 4.487\n",
    "Michael        0     * 4  +  0.147 * 3 = 0.441\n",
    "Joy            0.272 * 4  +  0.294 * 3 = 1.97\n",
    "Unicornelius   0.680 * 4  +  0.737 * 3 = 4.931\n",
    "```\n",
    "\n",
    "Then, we'll normalize that vector by dividing it by its norm. In the above example, the norm of the vector is $6.971$, so the new normalized vector is:\n",
    "```\n",
    "              Inside Out      Frozen 2             movie_profile\n",
    "Parth          0.680 * 4  +  0.589 * 3 = 4.487 ->      0.644 \n",
    "Michael        0     * 4  +  0.147 * 3 = 0.441 ->      0.063 \n",
    "Joy            0.272 * 4  +  0.294 * 3 = 1.97  ->      0.283\n",
    "Unicornelius   0.680 * 4  +  0.737 * 3 = 4.931 ->      0.707 \n",
    "```\n",
    "\n",
    "Notice that this vector is the same size as each of the movie vectors (it'll have 671 entries)... That hints towards the significance of the vector: we can think of it as a vector which represents the *perfect movie* for this user.\n",
    "\n",
    "The cosine similarity between two vectors $x = (x_1, x_2, \\dots, x_n)$ and $y = (y_1, y_2, \\dots, y_n)$ (which both have norm 1) is defined as their dot product, or the sum of element-wise products of their entries: $x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n$. This will be a number between 0 and 1 with higher values representing more similar vectors. You can think of the cosine similarity as an estimation of the \"closeness\" between the two vectors.\n",
    "\n",
    "Find the movies that are closest to our `movie_profile`: compute the cosine similarity between the `movie_profile` and each of the columns in our matrix and return the indices of the top `n` movies, in order from most similar to least similar.\n",
    "\n",
    "*Challenge: Try to implement this function without using any loops, using `numpy` broadcasting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 4.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 4.],\n",
       "       [0., 0., 4., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [3., 0., 0., 0., 0., 0., 0., 0., 0., 3.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [4., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[:10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_movies(user_ratings, normalized_ratings, n=5):\n",
    "    pass\n",
    "\n",
    "parth_ratings = {\n",
    "    8911: 5, # Inside Out\n",
    "    8460: 4, # Frozen 1\n",
    "    6294: 5  # Harry Potter and the Goblet of Fire\n",
    "}\n",
    "\n",
    "full_parth_ratings = np.array([parth_ratings.get(i, 0) for i in range(ratings.shape[1])])\n",
    "\n",
    "# As a sanity check, we've computed the value we got for the next line:\n",
    "suggest_movies(full_parth_ratings, normalized_ratings) # => array([8911, 6294, 8460, 5399, 8434])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my ratings, that corresponds to:\n",
    "```\n",
    "Inside Out\n",
    "Harry Potter and the Goblet of Fire\n",
    "Frozen 1\n",
    "Harry Potter and the Prisoner of Azkaban\n",
    "Ender's Game\n",
    "```\n",
    "\n",
    "Not bad! I haven't seen Ender's Game, so I guess that's on my list.\n",
    "\n",
    "Take a look at [movies.txt](movies.txt) and add in your own ratings!\n",
    "\n",
    "### So how does this work?\n",
    "Here's a fairly math-heavy explanation of how this is working. We're taking each movie and mapping it into 671-dimensional space, where each axis represents a different user's rating of that movie. We're assuming that each of those axes are orthoganal to one another (which, in reality, might not be a good assumption).\n",
    "\n",
    "Then, based on the inputted preferences, we're creating a new vector as a linear combination of the movie vectors that the inputted preferences have ranked. Then, we find which movies (vectors) are closest to that vector.\n",
    "\n",
    "This is called *cosine similarity* because of the standard formulation of the dot product. If $x = (x_1, x_2, \\dots, x_n)$ and $y = (y_1, y_2, \\dots, y_n)$, then:\n",
    "$$x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n = x \\cdot y = \\lVert x \\rVert \\lVert y \\rVert \\cos(\\theta)$$\n",
    "\n",
    "Where $\\theta$ is the angle between the two vectors. Since our vectors have norm 1, this simplifies:\n",
    "$$x_1 y_1 + x_2 y_2 + \\cdots + x_n y_n = \\cos(\\theta)$$\n",
    "\n",
    "By the definition of $\\cos$, this will give the length of the perpendicular distance between $x$ and $y$, which is why we use it as a measure of similarity between $x$ and $y$.\n",
    "\n",
    "### OMG this is so cool... I want to do more!\n",
    "Great! Here are some ideas for extensions and, potentially, a final project:\n",
    "1. Implement the matrix completion algorithm using [Carlos Fernandez-Granda's notes on low-rank matrix completion](https://cims.nyu.edu/~cfgranda/pages/OBDA_spring16/material/low_rank_models.pdf).\n",
    "2. Perform this analysis on a more complex data set from [Kaggle](https://www.kaggle.com/) or another dataset website.\n",
    "3. Take into account the fact that different users have similar preferences, and don't treat the axes as orthogonal.\n",
    "4. Perform unsupervised learning on this data set and cluster similar movies together...\n",
    "5. ...using that data, develop a Buzzfeed-style quiz where each question is of the form \"Which movie do you prefer more?\" and, based on the results of that quiz, determine one cluster of movies that the quiz-taker prefers, and recommend all of the movies from that cluster to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Standard Library\n",
    "\n",
    "We get it. At first, reading documentation doesn't sound like a fun way to spend an afternoon. However, this is one of a rare few times when you will have dedicated class time to take a deep dive into a library tool. Python's standard library is huge, and although your interests may not span the whole library, we're willing to bet that you can find something you enjoy in the library.\n",
    "\n",
    "Remember that you can follow along with the documentation's examples in the interactive interpreter - we recommend this approach, so that you're both reading about and practicing with the modules you like.\n",
    "\n",
    "Several of the documentation pages have links to the module's source code - if you're interested in seeing examples of well-crafted Python modules, there's no better place to look than the standard library!\n",
    "\n",
    "Above all, explore and ask questions!\n",
    "\n",
    "If you don't know which modules to look at, we have a list of some of our favorite modules that *weren't* covered in lecture, based on common general interests. Ask us about what you'd like to learn more about, and we'll point you in the right general direction.\n",
    "\n",
    "The top-level categories of tools in the standard library are:\n",
    "\n",
    "- Built-in [Functions](https://docs.python.org/3/library/functions.html), [Constants](https://docs.python.org/3/library/constants.html), [Types](https://docs.python.org/3/library/stdtypes.html), and [Exceptions](https://docs.python.org/3/library/exceptions.html)\n",
    "- [Text Processing Services](https://docs.python.org/3/library/text.html)\n",
    "- [Binary Data Services](https://docs.python.org/3/library/binary.html)\n",
    "- [Data Types](https://docs.python.org/3/library/datatypes.html)\n",
    "- [Numeric and Mathematical Modules](https://docs.python.org/3/library/numeric.html)\n",
    "- [Functional Programming Modules](https://docs.python.org/3/library/functional.html)\n",
    "- [File and Directory Access](https://docs.python.org/3/library/filesys.html)\n",
    "- [Data Persistence](https://docs.python.org/3/library/persistence.html)\n",
    "- [Data Compression and Archiving](https://docs.python.org/3/library/archiving.html)\n",
    "- [File Formats](https://docs.python.org/3/library/fileformats.html)\n",
    "- [Cryptographic Services](https://docs.python.org/3/library/crypto.html)\n",
    "- [Generic Operating System Services](https://docs.python.org/3/library/allos.html)\n",
    "- [Concurrent Execution](https://docs.python.org/3/library/concurrency.html)\n",
    "- [Context Variables](https://docs.python.org/3/library/contextvars.html)\n",
    "- [Networking and Interprocess Communication](https://docs.python.org/3/library/ipc.html)\n",
    "- [Internet Data Handling](https://docs.python.org/3/library/netdata.html)\n",
    "- [Structured Markup Processing Tools](https://docs.python.org/3/library/markup.html)\n",
    "- [Internet Protocols and Support](https://docs.python.org/3/library/internet.html)\n",
    "- [Multimedia Services](https://docs.python.org/3/library/mm.html)\n",
    "- [Internationalization](https://docs.python.org/3/library/i18n.html)\n",
    "- [Program Frameworks](https://docs.python.org/3/library/frameworks.html)\n",
    "- [Graphical User Interfaces with Tk](https://docs.python.org/3/library/tk.html)\n",
    "- [Development Tools](https://docs.python.org/3/library/development.html)\n",
    "- [Debugging and Profiling](https://docs.python.org/3/library/debug.html)\n",
    "- [Software Packaging and Distribution](https://docs.python.org/3/library/distribution.html)\n",
    "- [Python Runtime Services](https://docs.python.org/3/library/python.html)\n",
    "- [Custom Python Interpreters](https://docs.python.org/3/library/custominterp.html)\n",
    "- [Importing Modules](https://docs.python.org/3/library/modules.html)\n",
    "- [Python Language Services](https://docs.python.org/3/library/language.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Take Me To The Standard Library (Click Me!)](https://docs.python.org/3/library/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write\n",
    "\n",
    "In this section, you'll gain practice with some of the common modules in the Python standard library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating `collections`\n",
    "\n",
    "**Before continuing, read the [`collections` documentation](https://docs.python.org/3/library/collections.html) at least through the section on `namedtuple()`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with `collections.namedtuple`\n",
    "\n",
    "In this section, we modify code that prints out a message about each of a bunch of animals.\n",
    "\n",
    "Rewrite the following code to be more Pythonic by using `collections.namedtuple` to add readable attribute references. The attributes for these animals are `'name'`, `'species'`, `'color'`, and `'age'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lassie is an old black dog who is 12 years old.\n",
      "Buddy is a young red pupper who is 0.5 years old.\n",
      "Astro is an old grey doggo who is 15 years old.\n",
      "Mr. Peanutbutter is an old golden dog who is 35 years old.\n",
      "BoJack Horseman is a 52-year-old non-canine brown horse.\n",
      "Princess Carolyn is a 34-year-old non-canine pink cat.\n",
      "Mr. Tinkles is a 7-year-old non-canine white cat.\n",
      "Bella is a young brown pupper who is 0.5 years old.\n",
      "Max is a young brown doggo who is 5 years old.\n",
      "The Cat in the Hat is a 27-year-old non-canine stripey cat.\n",
      "Pluto (Disney) is a young orange dog who is 3 years old.\n",
      "Pluto (space) is a 4500000000-year-old non-canine brownish planet.\n",
      "Yertle is a 130-year-old non-canine green turtle.\n",
      "Horton is a 79-year-old non-canine blue elephant.\n"
     ]
    }
   ],
   "source": [
    "Animal = collections.namedtuple('Animal', ['name', 'species', 'color', 'age'])\n",
    "\n",
    "lassie = Animal('Lassie', 'dog', 'black', 12)\n",
    "buddy = Animal('Buddy', 'pupper', 'red', 0.5)\n",
    "astro = Animal('Astro', 'doggo', 'grey', 15)\n",
    "mrpb = Animal('Mr. Peanutbutter', 'dog', 'golden', 35)\n",
    "bojack = Animal('BoJack Horseman', 'horse', 'brown', 52)\n",
    "pc = Animal('Princess Carolyn', 'cat', 'pink', 34)\n",
    "tinkles = Animal('Mr. Tinkles', 'cat', 'white', 7)\n",
    "pupper = Animal('Bella', 'pupper', 'brown', 0.5)\n",
    "doggo = Animal('Max', 'doggo', 'brown', 5)\n",
    "seuss = Animal('The Cat in the Hat', 'cat', 'stripey', 27)\n",
    "pluto = Animal('Pluto (Disney)', 'dog', 'orange', 3)\n",
    "plu2o = Animal('Pluto (space)', 'planet', 'brownish', 4500000000)\n",
    "yertle = Animal('Yertle', 'turtle', 'green', 130)\n",
    "horton = Animal('Horton', 'elephant', 'blue', 79)\n",
    "\n",
    "animals = [lassie, buddy, astro, mrpb, bojack, pc, tinkles, pupper, doggo, seuss, pluto, plu2o, yertle, horton]\n",
    "\n",
    "for animal in animals:\n",
    "    name, species, color, age = animal[0], animal.species, animal[2], animal.age\n",
    "    if species in ('dog', 'doggo', 'pupper'):\n",
    "        if age > 5:\n",
    "            age_descriptor = 'an old'\n",
    "        else:\n",
    "            age_descriptor = 'a young'\n",
    "        print('{} is {} {} {} who is {} years old.'.format(name, age_descriptor, color, species, age))\n",
    "    else:\n",
    "        print('{} is a {}-year-old non-canine {} {}.'.format(name, age, color, species))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite me to be more Pythonic!\n",
    "lassie = ('Lassie', 'dog', 'black', 12)\n",
    "buddy = ('Buddy', 'pupper', 'red', 0.5)\n",
    "astro = ('Astro', 'doggo', 'grey', 15)\n",
    "mrpb = ('Mr. Peanutbutter', 'dog', 'golden', 35)\n",
    "bojack = ('BoJack Horseman', 'horse', 'brown', 52)\n",
    "pc = ('Princess Carolyn', 'cat', 'pink', 34)\n",
    "tinkles = ('Mr. Tinkles', 'cat', 'white', 7)\n",
    "pupper = ('Bella', 'pupper', 'brown', 0.5)\n",
    "doggo = ('Max', 'doggo', 'brown', 5)\n",
    "seuss = ('The Cat in the Hat', 'cat', 'stripey', 27)\n",
    "pluto = ('Pluto (Disney)', 'dog', 'orange', 3)\n",
    "plu2o = ('Pluto (space)', 'planet', 'brownish', 4500000000)\n",
    "yertle = ('Yertle', 'turtle', 'green', 130)\n",
    "horton = ('Horton', 'elephant', 'blue', 79)\n",
    "\n",
    "for animal in [lassie, buddy, astro, mrpb, bojack, pc, tinkles, pupper, doggo, seuss, pluto, plu2o, yertle, horton]:\n",
    "    if animal[1] == 'dog' or animal[1] == 'doggo' or animal[1] == 'pupper':\n",
    "        if animal[3] > 5:\n",
    "            print(animal[0] + ' is an old ' + animal[2] + ' ' + animal[1] + ' who is ' + str(animal[3]) + ' years old.')\n",
    "        else:\n",
    "            print(animal[0] + ' is a young ' + animal[2] + ' ' + animal[1] + ' who is ' + str(animal[3]) + ' years old.')\n",
    "    else:\n",
    "        print(animal[0] + ' is a ' + str(animal[3]) + '-year-old non-canine ' + animal[2] + ' ' + animal[1] + '.')\n",
    "        \n",
    "# Prints out:\n",
    "# Lassie is an old black dog who is 12 years old.\n",
    "# Buddy is a young red pupper who is 0.5 years old.\n",
    "# Astro is an old grey doggo who is 15 years old.\n",
    "# Mr. Peanutbutter is an old golden dog who is 35 years old.\n",
    "# BoJack Horseman is a 52-year-old non-canine brown horse.\n",
    "# Princess Carolyn is a 34-year-old non-canine pink cat.\n",
    "# Mr. Tinkles is a 7-year-old non-canine white cat.\n",
    "# Bella is a young brown pupper who is 0.5 years old.\n",
    "# Max is a young brown doggo who is 5 years old.\n",
    "# The Cat in the Hat is a 27-year-old non-canine stripey cat.\n",
    "# Pluto (Disney) is a young orange dog who is 3 years old.\n",
    "# Pluto (space) is a 4500000000-year-old non-canine brownish planet.\n",
    "# Yertle is a 130-year-old non-canine green turtle.\n",
    "# Horton is a 79-year-old non-canine blue elephant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `collections.defaultdict` and `collections.Counter`\n",
    "\n",
    "Using `/usr/share/dict/words` (alternatively, `https://stanfordpython.com/res/misc/words` if you are on Windows) as a data source, what are the three most common word lengths in the English language? Remember to strip off trailing whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bytes vs string in Python 3:**\n",
    "- Byte objects are in machine readable form internally, Strings are only in human readable form. Since Byte objects are machine readable, they can be directly stored on the disk. Whereas, Strings need encoding before which they can be stored on disk.\n",
    "https://stackoverflow.com/questions/6224052/what-is-the-difference-between-a-string-and-a-byte-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change me to another file location if you've downloaded a copy of the word list.\n",
    "# Recall that this file has one word per line.\n",
    "# FILENAME = '/usr/share/dict/words'\n",
    "URL = 'https://stanfordpython.com/res/misc/words'\n",
    "write_file = 'stanford_dict.txt'\n",
    "stanford_dict_response = requests.get(URL)\n",
    "if stanford_dict_response.ok:\n",
    "    raw_data = stanford_dict_response.content\n",
    "    with open(write_file, 'wb') as f:\n",
    "        f.write(raw_data)\n",
    "\n",
    "# TODO(you): Print the three most common word lengths in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 32403), (10, 30878), (8, 29989)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(write_file, 'r') as f:\n",
    "    raw_text = f.read()\n",
    "    word_len_counter = collections.Counter(list(map(lambda x: len(x), raw_text.split('\\n'))))\n",
    "\n",
    "MOST_COMMON_N = 3\n",
    "word_len_counter.most_common(MOST_COMMON_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evil Hangman Redux (optional)\n",
    "\n",
    "Feel free to skip this section if you aren't familiar with Keith Schwarz's CS106B/L assignment: \"Evil Hangman,\" in which a user plays the classic game of Hangman against a deceitful AI that will do everything that it can to win.\n",
    "\n",
    "Suppose that you have a function `mask(word, letter)` which replaces each character in `word` with a dash if that character is different than `letter` - for example, `mask('banana', 'a')  # => '-a-a-a'`. We've provided a sample implementation below.\n",
    "\n",
    "Your task is to write a function `largest_families(words, letter, num_families=3)` that returns the top `num_families` largest collections of words which share a mask, given a source collection of words and a chosen letter. In more detail, given a letter, a resulting family of words is one in which every word yields the same mask when masked using that letter. For example, suppose that `words = ['sees', 'says', 'sass']` and `letter = 's'`. Then there are two families of words: the mask `'s--s'` is a 2-word family containing `'sees'` and `'says'`, and the mask `'s-ss'` is a 1-word family containing just the word `'sass'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def mask(word, letter):\n",
    "    return ''.join('-' if letter != ch else letter for ch in word)\n",
    "\n",
    "\n",
    "def largest_families(words, letter, num_families=3):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Quick test\n",
    "words = ['sees', 'says', 'sass']\n",
    "print(largest_families(words, 's', num_families=1)[0])  # => Should print ['sees', 'says']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working Together\n",
    "\n",
    "Use tools from the `collections` module to implement an `Employee` database, which maintains organizational relationships among employees. Suppose that your data is provided in a tab-separated file:\n",
    "\n",
    "```\n",
    "employee_name    employee_manager    salary    department    title\n",
    "employee_name    employee_manager    salary    department    title\n",
    "...\n",
    "employee_name    employee_manager    salary    department    title\n",
    "```\n",
    "\n",
    "If you'd like sample data to work with, you can use the following\n",
    "```\n",
    "psarin    poohbear  0      CS   Instructor\n",
    "poohbear  sahami    500    CS   Lecturer\n",
    "tigger    poohbear  100    CS   Tiger\n",
    "htiek     sahami    500    CS   Lecturer\n",
    "sahami    mtl       5000   CS   Professor\n",
    "guido     guido     50000  PSF  BDFL\n",
    "```\n",
    "Save the above text to a file, making sure that your text editor doesn't automatically replace all of tabs with spaces!\n",
    "\n",
    "After writing code to load this information from a file, implement the following functions.\n",
    "\n",
    "```Python\n",
    "def directly_reports_to(employee, manager):\n",
    "    \"\"\"Return whether or not employee directly reports to manager\"\"\"\n",
    "    pass\n",
    "\n",
    "def indirectly_reports_to(employee, manager):\n",
    "    \"\"\"Return whether or not employee indirectly reports to manager\"\"\"\n",
    "    pass\n",
    "    \n",
    "def in_department(dept):\n",
    "    \"\"\"Return a collection of all employees of a given department\"\"\"\n",
    "    pass\n",
    "    \n",
    "def cost_of(dept):\n",
    "    \"\"\"Return the sum total of salaries for all employees of a given department\"\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "The primary portion of this section is parsing the file and storing the employees in a your choice of data structure keyed by some of the employees' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# Replace me with the name of a file containing employment data.\n",
    "FILENAME = 'employee.txt'\n",
    "\n",
    "# TODO(you): Read the data file and store the data in a data structure.\n",
    "Employee = collections.namedtuple('Employee', ['employee_name', 'employee_manager', 'salary', 'department', 'title'])\n",
    "employees = collections.defaultdict(Employee)\n",
    "with open(FILENAME, 'r') as f:\n",
    "    for row in f:\n",
    "        employee_name, employee_manager, salary, department, title = row.split('\\t')\n",
    "        employees[employee_name] = Employee(employee_name, employee_manager, salary, department, title)\n",
    "\n",
    "def directly_reports_to(employee, manager):\n",
    "    \"\"\"Return whether or not employee directly reports to manager\"\"\"\n",
    "    if employee in employees:\n",
    "        return employees[employee].employee_manager == manager\n",
    "    print('Employee {} not found'.format(employee))\n",
    "    return False\n",
    "\n",
    "\n",
    "def indirectly_reports_to(employee, manager):\n",
    "    \"\"\"Return whether or not employee indirectly reports to manager\"\"\"\n",
    "    top_level = False\n",
    "    while not top_level:\n",
    "        if employee not in employees:\n",
    "            print('Employee {} not found'.format(employee))\n",
    "            return False\n",
    "        direct_manager = employees[employee].employee_manager\n",
    "        if direct_manager == manager:\n",
    "            return True\n",
    "        elif employee == direct_manager: # only top level employee can have employee == direct_manager\n",
    "            return False\n",
    "        else:\n",
    "            employee = direct_manager\n",
    "\n",
    "\n",
    "def in_department(dept):\n",
    "    \"\"\"Return a collection of all employees of a given department\"\"\"\n",
    "    return list(filter(lambda x: employees[x].department == dept, employees.keys()))\n",
    "\n",
    "\n",
    "def cost_of(dept):\n",
    "    \"\"\"Return the sum total of salaries for all employees of a given department\"\"\"\n",
    "    return sum(map(lambda x: int(employees[x].salary) if employees[x].department == dept else 0, employees.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "Employee psarinssss not found\n",
      "False\n",
      "False\n",
      "True\n",
      "Employee mtl not found\n",
      "False\n",
      "['psarin', 'poohbear', 'tigger', 'htiek', 'sahami']\n",
      "[]\n",
      "50000\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# test functions\n",
    "print(directly_reports_to('psarin', 'poohbear')) # True\n",
    "print(directly_reports_to('poohbear', 'psarin')) # False\n",
    "print(directly_reports_to('psarinssss', 'poohbear')) # False\n",
    "print(directly_reports_to('psarin', 'sahami')) # False\n",
    "print(indirectly_reports_to('psarin', 'sahami')) # True\n",
    "print(indirectly_reports_to('psarin', 'htiek')) # False\n",
    "print(in_department('CS')) # psarin, poohbear, tigger, htiek, sahami\n",
    "print(in_department('PE')) # list()\n",
    "print(cost_of('PSF')) # 50000\n",
    "print(cost_of('PE')) # 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data with `re`\n",
    "\n",
    "If you're fairly new to regular expressions, we recommend you read through [the official Python HOWTO](https://docs.python.org/3/howto/regex.html) and walk through those examples instead of solving this portion of the lab.\n",
    "\n",
    "Otherwise, **read through the official [`re` documentation](https://docs.python.org/3/library/re.html) through \"Match Objects\"** (although the next section provides some neat examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordplay\n",
    "\n",
    "Using the list of words found at `/usr/share/dict/words` (or alternatively, `http://stanfordpython.com/res/misc/words`), determine all words that have all five vowels in order. That is, words that contain an `'a'`, `'e'`, `'i'`, `'o'`, and `'u'` in order, with any number (including 0) of non-vowel word characters before the 'a', between the vowels, and after the 'u'.\n",
    "\n",
    "For example, your list should contain both `\"abstemious\"` and `\"facetious\"`. We found a total of 14 matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Change me to another file location if you've downloaded a copy of the word list.\n",
    "# Recall that this file has one word per line.\n",
    "WORD_FILE = '/usr/share/dict/words'\n",
    "pattern = re.compile('your-regular-expression-here')\n",
    "\n",
    "# TODO(you): Print out any words that have five vowels in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### License Plates\n",
    "I love crosswords. Seriously. I do the NYTimes Mini every morning, pretty much as soon as I wake up. I also love thinking about crosswords and playing fun crossword-related games. One game that's well-known among cruciverbalists (folks who make crosswords) is the **license plate game**.\n",
    "\n",
    "Here's how you play the game: pick a license plate and ignore the numbers, filtering down to the alphabetic characters. Then, think of a word or phrase that has those characters, in that order. When you ignore numbers, my license plate is `\"btp\"`, so `\"breastplate\"` and `\"subtype\"` would be valid words.\n",
    "\n",
    "Using the list of words found at `/usr/share/dict/words` (or alternatively, `http://stanfordpython.com/res/misc/words`), write a function, `license_plate_words(letters)` which returns a list of words that contain `letters` in the given order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_FILE = '/usr/share/dict/words'\n",
    "\n",
    "def license_plate_words(letters):\n",
    "    pass\n",
    "\n",
    "print(license_plate_solver('btp')[:20])\n",
    "print(license_plate_solver('aeiou')) # this should be the same as the previous problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex Crossword Checker\n",
    "\n",
    "Take a moment to play one round of [Regex Crossword](https://regexcrossword.com/) (a highly entertaining site, if you've got hours to spare).\n",
    "\n",
    "In the spirit of Regex Crossword, we will write a function that checks arbitrary regex crosswords. Your function should take in two lists, one representing horizontal clues and one representing vertical clues, as well as the potential solution to crossword in the form a list-of-lists in row-major order (i.e. the elements are lists representing rows of the crossword. You should return whether or not the potential solution is in fact valid.\n",
    "\n",
    "```Python\n",
    "def regex_crossword_check(horizontal_patterns, vertical_patterns, candidate):\n",
    "    pass  # Your implementation here\n",
    "```\n",
    "\n",
    "For example, the call corresponding to the first \"Beginner\" puzzle (it's called \"Beatles\") would look like:\n",
    "\n",
    "```Python\n",
    "horiz = [r'HE|LL|O+', r'[PLEASE]+']\n",
    "vert = [r'[^SPEAK]+', r'EP|IP|EF']\n",
    "candidate = [\n",
    "    ['H', 'E'],\n",
    "    ['L', 'P']\n",
    "]\n",
    "regex_crossword_check(horiz, vert, candidate)  # => True\n",
    "```\n",
    "\n",
    "and the call corresponding to the second \"Experiences\" puzzle (it's called \"Royal Dinner\") would look like:\n",
    "\n",
    "```Python\n",
    "horiz = [r'(Y|F)(.)\\2[DAF]\\1', r'(U|O|I)*T[FRO]+', r'[KANE]*[GIN]*']\n",
    "vert = [r'(FI|A)+', r'(YE|OT)K', r'(.)[IF]+', r'[NODE]+', r'(FY|F|RG)+']\n",
    "candidate = [\n",
    "    ['F', 'O', 'O', 'D', 'F'],\n",
    "    ['I', 'T', 'F', 'O', 'R'],\n",
    "    ['A', 'K', 'I', 'N', 'G']\n",
    "]\n",
    "regex_crossword_check(horiz, vert, candidate)  # => True\n",
    "```\n",
    "\n",
    "Some implementation notes:\n",
    "\n",
    "* You may want to use `re.fullmatch` instead of `re.match` or `re.search`. The former matches a pattern string against an entire string, whereas the latter methods check to see if any prefix string or any substring, respectively, match the pattern.\n",
    "* You can get the width and height of the crossword from the length of the vertical and horizontal clue lists, respectively.\n",
    "* Remember your friend, `zip`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def regex_crossword_check(horizontal_patterns, vertical_patterns, candidate):\n",
    "    pass  # Your implementation \n",
    "\n",
    "\n",
    "# Quick tests.\n",
    "horiz = [r'HE|LL|O+', r'[PLEASE]+']\n",
    "vert = [r'[^SPEAK]+', r'EP|IP|EF']\n",
    "candidate = [\n",
    "    ['H', 'E'],\n",
    "    ['L', 'P']\n",
    "]\n",
    "print(regex_crossword_check(horiz, vert, candidate))  # => True\n",
    "\n",
    "\n",
    "horiz = [r'(Y|F)(.)\\2[DAF]\\1', r'(U|O|I)*T[FRO]+', r'[KANE]*[GIN]*']\n",
    "vert = [r'(FI|A)+', r'(YE|OT)K', r'(.)[IF]+', r'[NODE]+', r'(FY|F|RG)+']\n",
    "candidate = [\n",
    "    ['F', 'O', 'O', 'D', 'F'],\n",
    "    ['I', 'T', 'F', 'O', 'R'],\n",
    "    ['A', 'K', 'I', 'N', 'G']\n",
    "]\n",
    "print(regex_crossword_check(horiz, vert, candidate))  # => True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex Crossword Solver (challenge)\n",
    "\n",
    "This problem is hard - skip it unless you're feeling up for an algorithmic challenge.\n",
    "\n",
    "Write a function to solve arbitrary regular expression crosswords.\n",
    "\n",
    "Your function should take in two lists, one representing horizontal clues and one representing vertical clues, as well as a keyword argument representing the possible alphabet. Return (or lazily generate) a list of all answers consistent with the constraints, where an answer is formed by joining the characters in row-major order (consistent with their website).\n",
    "\n",
    "```Python\n",
    "import re\n",
    "import string\n",
    "def regex_crossword_solve(horizontal_patterns, vertical_patterns, alphabet=string.ascii_uppercase):\n",
    "    pass\n",
    "```\n",
    "\n",
    "For example, the call corresponding to the first \"Beginner\" puzzle (it's called \"Beatles\") would look like:\n",
    "\n",
    "```Python\n",
    "horiz = [r'HE|LL|O+', r'[PLEASE]+']\n",
    "vert = [r'[^SPEAK]+', r'EP|IP|EF']\n",
    "regex_crossword_solve(horiz, vert)\n",
    "```\n",
    "\n",
    "and would return the final answer `['HELP']` derived from the (unique, in this case) solution `[['H', 'E'], ['L', 'P']]`. If there are multiple answers, return them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def regex_crossword_solve(horizontal_patterns, vertical_patterns, alphabet=string.ascii_uppercase):\n",
    "    pass\n",
    "\n",
    "\n",
    "# Quick test.\n",
    "horiz = [r'HE|LL|O+', r'[PLEASE]+']\n",
    "vert = [r'[^SPEAK]+', r'EP|IP|EF']\n",
    "print(regex_crossword_solve(horiz, vert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multidirectional (super challenge)\n",
    "\n",
    "If you look though the Regex Crossword site linked above, you'll see that some puzzles (starting from \"Double Cross\" onwards), support multiple directions. Update your function above to work first with bidirection clues (as in \"Double Cross\", \"Cities\", \"Volapük\", and \"Hamlet\"). If you finish that, see if you can solve the types of puzzles shown in \"Hexagonal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def regex_crossword_solve_multidimensional(horizontal_patterns_lr, vertical_patterns_tb, horizontal_patterns_rl, vertical_patterns_bt, alphabet=string.ascii_uppercase):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimal Regex (super challenge)\n",
    "\n",
    "Given a finite set of positive samples and a finite set of negative examples, can we build a regular expression that matches the positives but rejects the negatives? Of course! We could just explicitly include the positives and explicitly reject the negatives. However, this approach leads to regexes that are quite long. For this part, write an algorithm that approximately generates the smallest regular expression that matches a list of positive samples and rejects a list of negative samples. Our metric for smallest will default to shortest, but feel free to come up with your own metric.\n",
    "\n",
    "*Note: this problem is NP-hard, and is tied to some deep results in complexity theory. For more information, check out [this CSTheory.SE post](http://cstheory.stackexchange.com/questions/1854/is-finding-the-minimum-regular-expression-an-np-complete-problem)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# This is a super challenging problem!\n",
    "def minimal_regex(positives, negatives):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `itertools`\n",
    "\n",
    "**Before continuing, make sure you read all of the [`itertools` documentation](https://docs.python.org/3/library/itertools.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabulation\n",
    "\n",
    "Write a `tabulate` function to generate a computation lookup table. `tabulate` should take in three arguments, a function, a start number (default 0), and a step size (default 1)\n",
    "\n",
    "```Python\n",
    "def tabulate(f, start=0, step=1):\n",
    "    pass\n",
    "```\n",
    "\n",
    "This function can be used as follows:\n",
    "\n",
    "```Python\n",
    "sqgen = tabulate(lambda x: x ** 2)\n",
    "next(sqgen)  # => 0 (which is equal to f(0))\n",
    "next(sqgen)  # => 1 (which is equal to f(1))\n",
    "next(sqgen)  # => 4 (which is equal to f(2))\n",
    "next(sqgen)  # => 9 (which is equal to f(3))\n",
    "```\n",
    "\n",
    "For reference, our implmentation is one line and 43 characters.\n",
    "\n",
    "Hint: take a look at the `itertools.count` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def tabulate(f, start=0, step=1):\n",
    "    return map(f, itertools.count(start, step))\n",
    "\n",
    "\n",
    "sqgen = tabulate(lambda x: x ** 2)\n",
    "print(next(sqgen))  # => 0 (which is equal to f(0))\n",
    "print(next(sqgen))  # => 1 (which is equal to f(1))\n",
    "print(next(sqgen))  # => 4 (which is equal to f(2))\n",
    "print(next(sqgen))  # => 9 (which is equal to f(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `random`\n",
    "\n",
    "**Before continuing, make sure you read the [`random` documentation](https://docs.python.org/3/library/random.html) through \"Functions for Sequences.\"**\n",
    "\n",
    "There's no code in this section - just read the documentation! It's rather short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `sys` for command-line tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition\n",
    "\n",
    "Write a Python script `add.py` (a new file) that can be run on the command line with any number of additional arguments representing numbers that you want to add up. Your script should print the sum of numeric arguments. If there are arguments that can't be converted to floats, ignore them. You can use what we learned about exceptional control flow to determine if a number is convertible to a float. If there are no additional arguments to your script, you should print an error message and exit.\n",
    "\n",
    "Recall you can use `sys.argv` to access the command-line arguments.\n",
    "\n",
    "You should be able to invoke your script from the command line as follows:\n",
    "\n",
    "```\n",
    "(cs41-env)$ python add.py 4 1\n",
    "5.0\n",
    "(cs41-env)$ python add.py 17 38 \"Hey wassup\" \"hello\"\n",
    "55.0\n",
    "(cs41-env)$ python add.py 8 6 7 5 3 0 9\n",
    "38.0\n",
    "(cs41-env)$ python add.py\n",
    "Usage: python add.py <nums>\n",
    "    \n",
    "    Add some numbers together\n",
    "```\n",
    "\n",
    "##### Argument Parsing with `argparse`\n",
    "\n",
    "Python's [`argparse` module](https://docs.python.org/3/library/argparse.html) provides a nicer way to define scripts that accept commmand-line arguments. Read through the `argparse` documentation and then rewrite the above program using the tools provided by `argparse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tree` (challenge)\n",
    "\n",
    "Write a program that emulates the command-line utility `tree`, which pretty-prints the directory structure rooted by an argument name. If there is no argument, use the current working directory. For example,\n",
    "\n",
    "```\n",
    "$ python3 tree.py python-labs/\n",
    "python-labs/\n",
    "├── LICENSE\n",
    "├── NOTES.md\n",
    "├── README.md\n",
    "├── markdown\n",
    "│   ├── lab1-warmup.md\n",
    "│   ├── lab2-datastructures.md\n",
    "│   ├── lab3-functions.md\n",
    "│   ├── lab4-fp.md\n",
    "│   ├── lab5-oop.md\n",
    "│   ├── lab6-standardlibrary.md\n",
    "│   ├── lab7-thirdparty.md\n",
    "│   └── lab8-pythonecosystem.md\n",
    "└── notebooks\n",
    "    ├── lab1-warmup-notebook.ipynb\n",
    "    ├── lab2-datastructures-notebook.ipynb\n",
    "    ├── lab3-functions-notebook.ipynb\n",
    "    ├── lab4-fp-notebook.ipynb\n",
    "    ├── lab5-oop-notebook.ipynb\n",
    "    ├── lab6-standardibrary-notebook.ipynb\n",
    "    ├── lab7-thirdparty-notebook.ipynb\n",
    "    └── lab8-pythonecosystem-notebook.ipynb\n",
    "```\n",
    "\n",
    "The above is just an example - don't worry if your actual `python-labs/` directory doesn't look like this.\n",
    "\n",
    "Use the [`pathlib` library](https://docs.python.org/3/library/pathlib.html) for filesystem navigation. For implementation details, check out `tree`'s [man page](http://linux.die.net/man/1/tree) or this [more helpful description](http://www.computerhope.com/unix/tree.htm). You don't need to implement any of the command-line flags for this part - just focus on navigating the file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improving `tree` (super challenge)\n",
    "\n",
    "Update your `tree` program to handle more advanced use cases, listed in the man page above. Can you handle symbolic links, maximum depth recursion, or pattern matching?\n",
    "\n",
    "You can make this tool as powerful as you'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Together Now\n",
    "\n",
    "This final problem will incorporate all of the modules we've seen so far. We'll build a tool to determine the shortest airport journey between any two airports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Data\n",
    "First, let's look at our data. OpenFlights publishes the following data files:\n",
    "\n",
    "* [Airlines](https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat)\n",
    "* [Airports](https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat)\n",
    "* [Routes](https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat)\n",
    "\n",
    "For information about the data itself, [DataHub](https://datahub.io/dataset/open-flights) has a good writeup on the schema.\n",
    "\n",
    "The information [by OpenFlights itself](https://openflights.org/data.html) is also quite good for getting an overview of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will write a script that, when given two airport codes (like SFO and JFK) and a maximum segment count, prints all possible ways to get from the source airport to the destination airport in at most that many segments:\n",
    "\n",
    "```\n",
    "$ python3 flights.py SFO JFK 2\n",
    "SFO -> JFK\n",
    "SFO -> LAX -> JFK\n",
    "SFO -> ORD -> JFK\n",
    "SFO -> DFW -> JFK\n",
    "...\n",
    "SFO -> PDX -> JFK\n",
    "```\n",
    "\n",
    "How powerful can you make this script? Consider adding extra features that utilize all of the standard library modules we've seen here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful wrote data from https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat to airlines.dat\n",
      "Successful wrote data from https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat to airports.dat\n",
      "Successful wrote data from https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat to routes.dat\n"
     ]
    }
   ],
   "source": [
    "# store data\n",
    "flights_data_loc = {'airlines': 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/airlines.dat', \n",
    "                    'airports': 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat', \n",
    "                    'routes': 'https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat'}\n",
    "file_ext = '.dat'\n",
    "\n",
    "def store_data_from_url(url, filename): \n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "            print('Successful wrote data from {} to {}'.format(url, filename))\n",
    "    return\n",
    "\n",
    "for k, v in flights_data_loc.items():\n",
    "    store_data_from_url(v, k + file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end, max_flights = 'SFO', 'JFK', 2\n",
    "\n",
    "Route = collections.namedtuple('Route', ['airline', 'airline_id', 'start_airport', 'end_airport', 'stops'])\n",
    "valid_routes = []\n",
    "\n",
    "with open('routes.dat', 'r') as f:\n",
    "    for line in f:\n",
    "        data = line.split(',')\n",
    "#         if data[2] == 'SFO' and data[4] == 'JFK':\n",
    "        valid_routes.append(Route(data[0], data[1], data[2], data[4], int(data[7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Route(airline='5T', airline_id='1623', start_airport='YRT', end_airport='YEK', stops=1), Route(airline='AC', airline_id='330', start_airport='ABJ', end_airport='BRU', stops=1), Route(airline='AC', airline_id='330', start_airport='YVR', end_airport='YBL', stops=1), Route(airline='CU', airline_id='1936', start_airport='FCO', end_airport='HAV', stops=1), Route(airline='FL', airline_id='1316', start_airport='HOU', end_airport='SAT', stops=1), Route(airline='FL', airline_id='1316', start_airport='MCO', end_airport='HOU', stops=1), Route(airline='FL', airline_id='1316', start_airport='MCO', end_airport='ORF', stops=1), Route(airline='SK', airline_id='4319', start_airport='ARN', end_airport='GEV', stops=1), Route(airline='WN', airline_id='4547', start_airport='BOS', end_airport='MCO', stops=1), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='BOS', stops=1), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='CAK', stops=1)]\n",
      "[Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='ALB', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='ATL', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='AUS', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='BDL', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='BHM', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='BNA', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='BOS', stops=1), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='BUF', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='BWI', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='CAK', stops=1), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='CLT', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='CMH', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='DAY', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='DEN', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='EYW', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='FNT', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='GRR', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='HOU', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='IND', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='ISP', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='JAN', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='LAS', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='MCI', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='MDW', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='MEM', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='MHT', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='MKE', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='MSP', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='MSY', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='ORF', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='PHL', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='PHX', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='PIT', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='PVD', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='RDU', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='RIC', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='ROC', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='SAT', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='SDF', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='SJU', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='STL', stops=0)]\n",
      "[Route(airline='WN', airline_id='4547', start_airport='ATL', end_airport='CAK', stops=0), Route(airline='WN', airline_id='4547', start_airport='BOS', end_airport='CAK', stops=0), Route(airline='WN', airline_id='4547', start_airport='DEN', end_airport='CAK', stops=0), Route(airline='WN', airline_id='4547', start_airport='LGA', end_airport='CAK', stops=0), Route(airline='WN', airline_id='4547', start_airport='MCO', end_airport='CAK', stops=1), Route(airline='WN', airline_id='4547', start_airport='TPA', end_airport='CAK', stops=0)]\n"
     ]
    }
   ],
   "source": [
    "# there are 11 routes that are not direct flights. this is obviously too low. \n",
    "print(list(filter(lambda x: x.stops > 0, valid_routes)))\n",
    "\n",
    "# remove these routes for now. it seems like the routes with stops = 1 have the individual stops = 0 flights anyways. we can get info on the transfer flights too using stops = 0 flights\n",
    "print(list(filter(lambda x: x.airline == 'WN' and x.start_airport == 'MCO', valid_routes)))\n",
    "print(list(filter(lambda x: x.airline == 'WN' and x.end_airport == 'CAK', valid_routes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = []\n",
    "Route = collections.namedtuple('Route', ['airline', 'airline_id', 'start_airport', 'end_airport', 'stops'])\n",
    "\n",
    "with open('routes.dat', 'r') as f:\n",
    "    for line in f:\n",
    "        data = line.split(',')\n",
    "        routes.append(Route(data[0], data[1], data[2], data[4], int(data[7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random_routes = random.choices(routes, k = len(routes)//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_routes(start_airport, end_airport, max_flights, route_map):\n",
    "    \n",
    "    valid_routes = []\n",
    "    memoize_routes = {} # key = (start, end, max_flights), value = set(routes from start to end within max_flights Route(airline, airline_id, flight_cnt))\n",
    "    \n",
    "    def get_valid_routes_recur(start, end, max_flights, route_map, curr_flights):\n",
    "        curr_start, curr_end, curr_flight_cnt = curr_flights[0].start_airport, curr_flights[-1].end_airport, len(curr_flights)\n",
    "        if curr_flight_cnt > max_flights: \n",
    "            return\n",
    "        if curr_start == start and curr_end == end:\n",
    "            valid_routes.append(curr_flights)\n",
    "            return\n",
    "        for route in route_map:\n",
    "            if route.start_airport == curr_end:\n",
    "                get_valid_routes_recur(start, end, max_flights, route_map, curr_flights + [route])\n",
    "        return\n",
    "    \n",
    "    for route in route_map:\n",
    "        if route.start_airport == start_airport:\n",
    "            get_valid_routes_recur(start_airport, end_airport, max_flights, route_map, [route])\n",
    "    return valid_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Route(airline='VX', airline_id='5331', start_airport='SFO', end_airport='ORD', stops=0),\n",
       " Route(airline='DL', airline_id='2009', start_airport='ORD', end_airport='JFK', stops=0)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = get_valid_routes('SFO', 'JFK', 2, random_routes)\n",
    "print(len(t))\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.730555223100236"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_code = '''\n",
    "import collections\n",
    "import itertools\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "routes = []\n",
    "Route = collections.namedtuple('Route', ['airline', 'airline_id', 'start_airport', 'end_airport', 'stops'])\n",
    "\n",
    "with open('routes.dat', 'r') as f:\n",
    "    for line in f:\n",
    "        data = line.split(',')\n",
    "        routes.append(Route(data[0], data[1], data[2], data[4], int(data[7])))\n",
    "\n",
    "random.seed(0)\n",
    "random_routes = random.choices(routes, k = len(routes)//10)\n",
    "\n",
    "def get_valid_routes(start_airport, end_airport, max_flights, route_map):\n",
    "    \n",
    "    valid_routes = []\n",
    "    memoize_routes = {} # key = (start, end, max_flights), value = set(routes from start to end within max_flights Route(airline, airline_id, flight_cnt))\n",
    "    \n",
    "    def get_valid_routes_recur(start, end, max_flights, route_map, curr_flights):\n",
    "        curr_start, curr_end, curr_flight_cnt = curr_flights[0].start_airport, curr_flights[-1].end_airport, len(curr_flights)\n",
    "        if curr_flight_cnt > max_flights: \n",
    "            return\n",
    "        if curr_start == start and curr_end == end:\n",
    "            valid_routes.append(curr_flights)\n",
    "            return\n",
    "        for route in route_map:\n",
    "            if route.start_airport == curr_end:\n",
    "                get_valid_routes_recur(start, end, max_flights, route_map, curr_flights + [route])\n",
    "        return\n",
    "    \n",
    "    for route in route_map:\n",
    "        if route.start_airport == start_airport:\n",
    "            get_valid_routes_recur(start_airport, end_airport, max_flights, route_map, [route])\n",
    "    return valid_routes\n",
    "'''\n",
    "test_code = '''get_valid_routes('SFO', 'JFK', 3, random_routes)'''\n",
    "test_number = 10\n",
    "test_time = timeit.timeit(stmt = test_code, setup = setup_code, number = test_number)\n",
    "test_time / test_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with memoization\n",
    "# memoization is hard to implement because 1) update rules can be very time-inefficient, 2) you don't want to recommend flights that go over max_flights, 3) you don't want to recommend itineraries that return a user to a previously visited airport (this one is a concern depending on implementation)\n",
    "\n",
    "def get_valid_routes(start_airport, end_airport, max_flights, route_map):\n",
    "    \n",
    "    valid_routes = []\n",
    "    memoize_routes = collections.defaultdict(list) # key = (start, end, max_flights), value = set(routes from start to end within max_flights Route(airline, airline_id, flight_cnt))\n",
    "    \n",
    "    def get_valid_routes_recur(start, end, max_flights, route_map, curr_flights):\n",
    "        curr_start, curr_end, curr_flight_cnt = curr_flights[0].start_airport, curr_flights[-1].end_airport, len(curr_flights)\n",
    "        if curr_flight_cnt > max_flights: \n",
    "            return\n",
    "        if (curr_end, end) in memoize_routes:\n",
    "            valid_routes.extend(map(lambda x: curr_flights + x, memoize_routes[(curr_end, end)]))\n",
    "            for i, route in enumerate(route_map):\n",
    "                memoize_routes[(route.start_airport, end)].append()\n",
    "            return\n",
    "        if curr_start == start and curr_end == end:\n",
    "            valid_routes.append(curr_flights)\n",
    "            for i, route in enumerate(route_map):\n",
    "                memoize_routes[(route.start_airport, end)].append(route_map[i:])\n",
    "            return\n",
    "        for route in route_map:\n",
    "            if route.start_airport == curr_end:\n",
    "                get_valid_routes_recur(start, end, max_flights, route_map, curr_flights + [route])\n",
    "        return\n",
    "    \n",
    "    for route in route_map:\n",
    "        if route.start_airport == start_airport:\n",
    "            get_valid_routes_recur(start_airport, end_airport, max_flights, route_map, [route])\n",
    "    return valid_routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[['SFO', 'ATL', 'JFK'], ['SFO', 'ORD', 'JFK'], ['SFO', 'SAL', 'JFK'], ['SFO', 'LAS', 'JFK'], ['SFO', 'YYZ', 'JFK'], ['SFO', 'PEK', 'JFK'], ['SFO', 'FRA', 'JFK'], ['SFO', 'SAN', 'JFK']]\n",
      "[['SFO', 'ATL', 'JFK'], ['SFO', 'ORD', 'JFK'], ['SFO', 'SAL', 'JFK'], ['SFO', 'LAS', 'JFK'], ['SFO', 'YYZ', 'JFK'], ['SFO', 'PEK', 'JFK'], ['SFO', 'FRA', 'JFK'], ['SFO', 'SAN', 'JFK'], ['SFO', 'CLT', 'ATL', 'JFK'], ['SFO', 'CLT', 'SJO', 'JFK'], ['SFO', 'CLT', 'SAV', 'JFK'], ['SFO', 'CLT', 'BNA', 'JFK'], ['SFO', 'CLT', 'MSY', 'JFK'], ['SFO', 'CLT', 'TPA', 'JFK'], ['SFO', 'ATL', 'MEX', 'JFK'], ['SFO', 'ATL', 'CHS', 'JFK'], ['SFO', 'ATL', 'FRA', 'JFK'], ['SFO', 'ATL', 'BNA', 'JFK'], ['SFO', 'ATL', 'SAN', 'JFK'], ['SFO', 'ATL', 'ORD', 'JFK'], ['SFO', 'ATL', 'IND', 'JFK'], ['SFO', 'ATL', 'SAL', 'JFK'], ['SFO', 'ATL', 'SAT', 'JFK'], ['SFO', 'ATL', 'CMH', 'JFK'], ['SFO', 'ORD', 'ATL', 'JFK'], ['SFO', 'ORD', 'MEX', 'JFK'], ['SFO', 'ORD', 'MSY', 'JFK'], ['SFO', 'ORD', 'SAT', 'JFK'], ['SFO', 'ORD', 'TPA', 'JFK'], ['SFO', 'CDG', 'MEX', 'JFK'], ['SFO', 'CDG', 'PEK', 'JFK'], ['SFO', 'CDG', 'NCE', 'JFK'], ['SFO', 'CDG', 'ORD', 'JFK'], ['SFO', 'CDG', 'YYZ', 'JFK'], ['SFO', 'CDG', 'JNB', 'JFK'], ['SFO', 'DXB', 'DOH', 'JFK'], ['SFO', 'DXB', 'PEK', 'JFK'], ['SFO', 'DXB', 'HKG', 'JFK'], ['SFO', 'DXB', 'YYZ', 'JFK'], ['SFO', 'DUB', 'BRU', 'JFK'], ['SFO', 'DUB', 'NCE', 'JFK'], ['SFO', 'HNL', 'ORD', 'JFK'], ['SFO', 'LAS', 'SAN', 'JFK'], ['SFO', 'LAS', 'MEX', 'JFK'], ['SFO', 'YYZ', 'ATL', 'JFK'], ['SFO', 'YYZ', 'HKG', 'JFK'], ['SFO', 'YYZ', 'IND', 'JFK'], ['SFO', 'YYZ', 'BRU', 'JFK'], ['SFO', 'NRT', 'PEK', 'JFK'], ['SFO', 'SLC', 'ORD', 'JFK'], ['SFO', 'SLC', 'MSY', 'JFK'], ['SFO', 'SNA', 'ORD', 'JFK'], ['SFO', 'SNA', 'LAS', 'JFK'], ['SFO', 'BJX', 'ATL', 'JFK'], ['SFO', 'LHR', 'ATL', 'JFK'], ['SFO', 'LHR', 'CAI', 'JFK'], ['SFO', 'LHR', 'ORD', 'JFK'], ['SFO', 'LHR', 'EZE', 'JFK'], ['SFO', 'LHR', 'FRA', 'JFK'], ['SFO', 'PHL', 'MCO', 'JFK'], ['SFO', 'PHL', 'ATL', 'JFK'], ['SFO', 'PHL', 'IND', 'JFK'], ['SFO', 'PHL', 'DOH', 'JFK'], ['SFO', 'PHL', 'SAV', 'JFK'], ['SFO', 'PHL', 'BNA', 'JFK'], ['SFO', 'PHL', 'TPA', 'JFK'], ['SFO', 'PHL', 'FRA', 'JFK'], ['SFO', 'PEK', 'CAI', 'JFK'], ['SFO', 'PEK', 'ORD', 'JFK'], ['SFO', 'PEK', 'FRA', 'JFK'], ['SFO', 'YYC', 'LAS', 'JFK'], ['SFO', 'YYC', 'SAN', 'JFK'], ['SFO', 'FRA', 'ATL', 'JFK'], ['SFO', 'FRA', 'DOH', 'JFK'], ['SFO', 'FRA', 'CAI', 'JFK'], ['SFO', 'FRA', 'FCO', 'JFK'], ['SFO', 'FRA', 'JNB', 'JFK']]\n"
     ]
    }
   ],
   "source": [
    "# solution's version\n",
    "\n",
    "import csv\n",
    "\n",
    "Airport = collections.namedtuple('Airport', ['id', 'name', 'city', 'country', 'faa_iata', 'icao', 'lat', 'long', 'alt', 'utc_offset', 'dst', 'tz', 'type', 'source'])\n",
    "Airline = collections.namedtuple('Airline', ['id', 'name', 'alias', 'iata', 'icao', 'callsign', 'country', 'active'])\n",
    "Route = collections.namedtuple('Route', ['airline', 'airline_id', 'source_airport', 'source_airport_id', 'dest_airport', 'dest_airport_id', 'codeshare', 'stops', 'equipment'])\n",
    "\n",
    "def load_data():\n",
    "    with open('airports.dat') as f:\n",
    "        airports = {}\n",
    "        for line in csv.reader(f):\n",
    "            airport = Airport._make(line)\n",
    "            airports[airport.id] = airport\n",
    "\n",
    "    with open('airlines.dat') as f:\n",
    "        airlines = {}\n",
    "        for line in csv.reader(f):\n",
    "            airline = Airline._make(line)\n",
    "            airlines[airline.id] = airline\n",
    "\n",
    "    with open('routes.dat') as f:\n",
    "        # top-level keyed by source airport ID, next level keyed by destination airport ID\n",
    "        routes = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "        for line in csv.reader(f):\n",
    "            route = Route._make(line)\n",
    "            routes[route.source_airport][route.dest_airport].append(route)\n",
    "\n",
    "    return airports, airlines, routes\n",
    "\n",
    "def find_flights(routes, source_airport, destination_airport, max_segments):\n",
    "    # We implement a basic BFS algorithm for following the routes\n",
    "    # Taken from http://eddmann.com/posts/depth-first-search-and-breadth-first-search-in-python/\n",
    "    queue = [(source_airport, [source_airport])]\n",
    "    while queue:\n",
    "        airport, path = queue.pop(0)\n",
    "        if len(path) > max_segments:\n",
    "            return\n",
    "        for next_airport in set(routes[airport].keys()) - set(path):\n",
    "            if next_airport == destination_airport:\n",
    "                yield path + [next_airport]\n",
    "            else:\n",
    "                queue.append((next_airport, path + [next_airport]))\n",
    "\n",
    "solution_random_routes = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "for r in random_routes:\n",
    "    solution_random_routes[r.start_airport][r.end_airport].append(r)\n",
    "print(list(find_flights(solution_random_routes, 'SFO', 'JFK', 1)))\n",
    "print(list(find_flights(solution_random_routes, 'SFO', 'JFK', 2)))\n",
    "print(list(find_flights(solution_random_routes, 'SFO', 'JFK', 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cute Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `turtle` - Turtle graphics\n",
    "\n",
    "Run the following code. A graphical window should appear that shows your new turtle friend! What other interesting shapes can you make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turtle\n",
    "\n",
    "turtle.left(180)\n",
    "turtle.forward(200)\n",
    "turtle.left(180)\n",
    "\n",
    "turtle.color('red', 'yellow')\n",
    "turtle.begin_fill()\n",
    "\n",
    "for _ in range(36):\n",
    "    turtle.forward(400)\n",
    "    turtle.left(170)\n",
    "    if abs(turtle.pos()) < 1:\n",
    "        break\n",
    "\n",
    "turtle.end_fill()\n",
    "turtle.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `unicodedata` - Unicode Database\n",
    "\n",
    "Think about your favorite emoji. Can you guess its official name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "print(unicodedata.lookup('SLICE OF PIZZA'))  # 🍕\n",
    "\n",
    "print(unicodedata.name('🦄'))  # UNICORN FACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `this` and `antigravity`\n",
    "\n",
    "Just run the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antigravity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Semantics\n",
    "\n",
    "If you've made it through this far, congratulations! This was a long lab. If you're interested in the nitty-gritty details of Python's import mechanics, you can read through the [specification of the import system in the official language reference](https://docs.python.org/3/reference/import.html). It's a fairly long read but it can precisely answer any lingering questions you might have about exactly how Python imports modules and packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit\n",
    "Credit to Sam Redmond (@sredmond) who designed many of the Standard Library/Third-Party Library problems in this lab. Credit also goes to some video/person that Parth watched/talked to about crosswords which he can't remember any more... &#128542;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With &#129412;s by @psarin and @coopermj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
